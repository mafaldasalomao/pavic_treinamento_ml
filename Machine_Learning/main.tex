\documentclass{beamer}
\usetheme{Madrid}
\usecolortheme{default}
\usepackage{amsmath,amssymb,lmodern}
\usepackage[thicklines]{cancel}
\usepackage{caption}
\usepackage{listings}
\usepackage{animate}
\usepackage{subcaption}
\usepackage{xcolor}
\usepackage{graphicx} %Loading the package

\graphicspath{{figures/}} %Setting the graphicspath
\title[Machine Learning]
{Treinamento de Machine Learning e Deep Learning}

\subtitle{Do Básico ao Avançado}

\author[Mafalda, Salomão] % (optional, for multiple authors)
{Salomão Machado Mafalda\inst{1}}

\institute[PAVIC] % (optional)
{
  \inst{1}%
  Universidade Federal do Acre\\
  PAVIC

}

\date[2023] % (optional)
{2023}

\logo{\includegraphics[height=0.8cm]{ente}}

\definecolor{uoftblue}{RGB}{6,41,88}
\setbeamercolor{titlelike}{bg=uoftblue}
\setbeamerfont{title}{series=\bfseries}

\begin{document}

\frame{\titlepage}

\begin{frame}
\frametitle{Agenda}
\tableofcontents
\end{frame}



%==========================================================================================
\section{Perceptron}

\begin{frame}
	\frametitle{Perceptron}
	\begin{figure}
		\centering
		\label{fig:neuron}
		\includegraphics[width=0.4\linewidth]{neuron}
		\caption{Neurônio humano}
	\end{figure}
\end{frame}


%==========================================================================================
\begin{frame}
	\frametitle{Perceptron}
	\begin{figure}
		\centering
		\includegraphics[width=0.4\linewidth]{figures/neuron_ai}
		\caption{Neurônio Artificial}
		\label{fig:neuronai}
	\end{figure}
\end{frame}

%==========================================================================================
\begin{frame}
	\frametitle{Perceptron}
	\begin{figure}
		\centering
		\begin{subfigure}{.45\textwidth}
			\includegraphics[width=1\linewidth]{figures/neuron}
			\caption{Neurônio Humano}
		\end{subfigure}
		\begin{subfigure}{.45\textwidth}
			\includegraphics[width=1\linewidth]{figures/neuron_ai}
			\caption{Neurônio Artificial}
		\end{subfigure}
	\end{figure}
\end{frame}
%==========================================================================================
\begin{frame}
	\frametitle{Perceptron}
	\begin{itemize}
		\item Modelo mais básico de NN
		\item Um neurônio
		\item N entradas, Uma saída ŷ
	\end{itemize}
	\begin{figure}
		\centering
		\includegraphics[width=0.4\linewidth]{figures/neuron_ai}
		\caption{Neurônio Artificial}
	\end{figure}

	\begin{gather*}
		\hat{y} = f( \sum_i w_i x_i + b)
	\end{gather*}
\end{frame}
%==========================================================================================
\begin{frame}
	\frametitle{Perceptron}
	\begin{itemize}
		\item Modelo mais básico de NN
		\item Um neurônio
		\item N entradas, Uma saída ŷ
		\item Classificador binário linear
		\item Pode ser usado para Regressão
		\item Perceptron Rule
		\item Aprendizado Online
		\begin{itemize}
			\item Atualiza os pesos por amostra
		\end{itemize}
	\end{itemize}
	\begin{figure}
		\centering
		\includegraphics[width=0.6\linewidth]{figures/linear_regression}
	\end{figure}
	
	\begin{gather*}
		\hat{y} = f( \sum_i w_i x_i + b)
	\end{gather*}
\end{frame}
%==========================================================================================
\begin{frame}
	\frametitle{Perceptron}
	\begin{block}{Função de ativação do perceptron}
		$$
		\left\{\begin{matrix}
			0 \; if \; 0 > x
			\\ 
			1 \; if \; x \geq 0
		\end{matrix}\right.$$

		\begin{itemize}
			\item 0 se for negativo
			\item 1 se maior ou igual a 0
		\end{itemize}
	\end{block}
	
	\begin{figure}
		\centering
		\includegraphics[width=0.35\linewidth]{figures/step_function}
	\end{figure}
\end{frame}
%==========================================================================================

\begin{frame}
	\frametitle{Perceptron}
		\begin{block}{Perceptron Rule}
			O perceptron atualiza seus pesos utilizando a perceptron rule, não com o gradiente
	\end{block}
Atualização dos pesos:
	\begin{gather*}
		w_i = w_i + \lambda*(y_i -  \hat{y}_i)* x_i
	\end{gather*}
	Atualização do \textit{bias}:
	\begin{gather*}
		b_i = b_i + \lambda*(y_i -  \hat{y}_i)
	\end{gather*}
\begin{alertblock}{Observação importante}
``Quando a diferença yi -  ŷi for 0 então não ocorrerá a atualização dos pesos''
\end{alertblock}

\end{frame}
%==========================================================================================

%==========================================================================================
\begin{frame}
	\frametitle{Perceptron}
	\begin{block}{Ponto de partida diferente}
		Com diferentes pontos de partida, o algoritmo encontra quase a mesma solução, embora com diferentes taxas de convergência.
		
		\href{https://github.com/mafaldasalomao/pavic_treinamento_ml/blob/main/Machine_Learning/figures/random_01.gif?raw=true}{\beamergotobutton{Caso 01}} \\
		\href{https://github.com/mafaldasalomao/pavic_treinamento_ml/blob/main/Machine_Learning/figures/random_02.gif?raw=true}{\beamergotobutton{Caso 02}}
	\end{block}
	\begin{block}{Learning Rate - Taxa de aprendizagem}
		\begin{itemize}
			\item LR = 0.01 a velocidade de convergência é muito lenta.
			Quando o cálculo se torna complicado, uma taxa de aprendizado muito baixa afetará a velocidade do algoritmo, mesmo nunca atingindo o destino.
			\item LR = 0.5, o algoritmo se aproxima do alvo muito rapidamente após várias iterações. No entanto, o algoritmo falha ao convergir porque o salto é muito grande, fazendo com que ele fique parado no destino.
		\end{itemize}
		\href{https://github.com/mafaldasalomao/pavic_treinamento_ml/blob/main/Machine_Learning/figures/lr_01.gif}{\beamergotobutton{Caso 01}} \\
		\href{https://github.com/mafaldasalomao/pavic_treinamento_ml/blob/main/Machine_Learning/figures/lr_02.gif}{\beamergotobutton{Caso 02}}
	\end{block}
\end{frame}
%==========================================================================================
\begin{frame}
	\frametitle{Perceptron}
	\begin{block}{Vamos ver na prática}
		Vamos praticar utilizando o notebook 00\_perceptron
	\end{block}
\end{frame}

%==========================================================================================


%==========================================================================================
\section{Adaline}


%==========================================================================================
\begin{frame}
	\frametitle{Adaline}
	\begin{figure}
		\centering
		\includegraphics[width=0.4\linewidth]{figures/neuron_ai}
		\caption{Neurônio Artificial}
		\label{fig:neuronai_ada}
	\end{figure}
\end{frame}

%==========================================================================================
%==========================================================================================
%==========================================================================================
\begin{frame}
	\frametitle{Adaline}
	\begin{itemize}
		\item Modelo mais básico de NN
		\item Um neurônio
		\item N entradas, Uma saída ŷ
		\item Classificador binário linear
		\item Pode ser usado para Regressão
		\item Sabe o quanto 'errou'
		\item Aplica-se o gradiente descendente
		\item Aprendizado Online
	\end{itemize}
	
	\begin{gather*}
		\hat{y} = f( \sum_i w_i x_i + b)
	\end{gather*}
\end{frame}
%==========================================================================================
\begin{frame}
	\frametitle{Adaline}
	\begin{block}{Função de ativação do Adaline}
		$$f(x) = x$$
		\begin{itemize}
			\item Possibilita o cálculo da derivada
		\end{itemize}
	\end{block}
	
	\begin{figure}
		\centering
		\includegraphics[width=0.35\linewidth]{figures/linear_function}
	\end{figure}

\end{frame}
%==========================================================================================

\begin{frame}
	\frametitle{Adaline vs Perceptron}
	\begin{figure}
		\centering
		\includegraphics[width=0.7\linewidth]{figures/comparacao_perceptron_adaline}
	\end{figure}
\end{frame}
%==========================================================================================
\begin{frame}
	\frametitle{Adaline vs Perceptron}
	\begin{figure}
		\centering
		\includegraphics[width=0.9\linewidth]{figures/hiperplanos_perceptron_adaline}
	\end{figure}

\end{frame}
%==========================================================================================
\begin{frame}
	\frametitle{Adaline}
	\begin{block}{Como atualizar os pesos do Adaline}
			\begin{gather*}
			w_i = w_i - \lambda * (y_i - \hat{y}_i) * x_i
		\end{gather*}
	O erro predito será a saída predita menos a saída desejada multiplicados pela entrada (x)
		\begin{gather*}
			J(w) = \frac{1}{2} \sum_{i}^{N} (y_i - \hat{y}_i)^2 \\ 
			\frac{\partial J}{\partial w_i} = \frac{\partial}{\partial w_i} \frac{1}{2} \sum_{i}^{N} (y_i - \hat{y}_i)^2 = \frac{1}{2} \sum_{i}^{N}\frac{\partial}{\partial w_i} (y_i - \hat{y}_i)^2  \\
			= \sum_{i}^{N}(y_i - \hat{y}_i)\frac{\partial}{\partial w_i} (y_i - \hat{y}_i) = \sum_{i}^{N}(y_i - \hat{y}_i) (x_i) \to \frac{\partial J}{\vec{w}} = - (\vec{y}- \vec{\hat{y}})\vec{x}
		\end{gather*}
	\end{block}

		
\end{frame}
%==========================================================================================
\begin{frame}
	\frametitle{Adaline}
	\begin{block}{Vamos ver na prática}
		Vamos praticar utilizando o notebook 01\_adaline
	\end{block}
\end{frame}

%==========================================================================================

\section{Neurônio Sigmoide}


%==========================================================================================
\begin{frame}
	\frametitle{Neurônio Sigmoide}
	\begin{figure}
		\centering
		\includegraphics[width=0.4\linewidth]{figures/neuron_ai}
		\caption{Neurônio Artificial}
		\label{fig:neuronai_sig}
	\end{figure}
\end{frame}

%==========================================================================================
%==========================================================================================
%==========================================================================================
\begin{frame}
	\frametitle{Neurônio Sigmoide}
	\begin{itemize}
		\item Modelo mais básico de NN
		\item Um neurônio
		\item N entradas, Uma saída ŷ
		\item Custo: Entropia Cruzada
		\item Classificação binária não-linear
		\item Pequenas alterações nos parâmetros geram pequenas alterações nas saídas
		\item Sabe o quanto 'errou'
		\item Aplica-se o gradiente descendente
	\end{itemize}
	
	\begin{gather*}
		\hat{y} = f( \sum_i w_i x_i + b)
	\end{gather*}
\end{frame}
%==========================================================================================
\begin{frame}
	\frametitle{Neurônio Sigmoide}
	\begin{block}{Função de ativação do Neurônio Sigmoide}
		$$f(x) = \frac{1}{1-e^{-x}}$$
		\begin{itemize}
			\item Possibilita o cálculo da derivada em todos pontos
			\item Aplicado em problemas de regressão logística
		\end{itemize}
	\end{block}
	
\begin{figure}
	\centering
	\includegraphics[width=0.35\linewidth]{figures/sigmoid_function}
\end{figure}

	
\end{frame}
%==========================================================================================

\begin{frame}
	\frametitle{Neurônio Sigmoide}
	\begin{block}{Entropia Cruzada}
		$p_j$ é o valor a ser predito e $t_j$ é o valor predito
		\begin{gather*}
			L = - \frac{1}{N} \left [ \sum_{j=1}^{N} [t_j log(p_j) + (1-t_j)log(1-p_j)] \right ] 
		\end{gather*}
	\end{block}
\end{frame}
%==========================================================================================

\begin{frame}
	\frametitle{Neurônio Sigmoide}
	Vamos tomar:
	% Please add the following required packages to your document preamble:
	% \usepackage[table,xcdraw]{xcolor}
	% If you use beamer only pass "xcolor=table" option, i.e. \documentclass[xcolor=table]{beamer}
	\begin{table}[]
		\begin{tabular}{|l|l|l|l|}
			\hline
			$p_j$ & $t_j$ & Erro  & $L$ \\ \hline
			0  & 0  & 0 - 0 = 0  & \textcolor{green}{0}  \\ \hline
			0  & 1  & 0 - 1 = -1 & $\infty$ \\ \hline
			1  & 0  & 1 - 0 = 1  & $\infty$ \\ \hline
			1  & 1  & 1 - 1 = 0  & 0 \\ \hline
		\end{tabular}
	\end{table}
	\begin{block}{Entropia Cruzada}
		Para entrada $0, 0$ e saída predita $\textcolor{red}{0}$ e a saída desejada for $\textcolor{blue}{0}$
		\begin{gather*}
			L = - \frac{1}{N} \left [ \sum_{j=1}^{N} [\textcolor{red}{0} log(\textcolor{blue}{0} ) + (1-\textcolor{red}{0})log(1-\textcolor{blue}{0} )] \right ] 
		\end{gather*}
	\end{block}
\end{frame}
%==========================================================================================
\begin{frame}
	\frametitle{Neurônio Sigmoide}
	Vamos tomar:
	% Please add the following required packages to your document preamble:
	% \usepackage[table,xcdraw]{xcolor}
	% If you use beamer only pass "xcolor=table" option, i.e. \documentclass[xcolor=table]{beamer}
	\begin{table}[]
		\begin{tabular}{|l|l|l|l|}
			\hline
			$p_j$ & $t_j$ & Erro  & $L$ \\ \hline
			0  & 0  & 0 - 0 = 0  & \textcolor{green}{0}  \\ \hline
			0  & 1  & 0 - 1 = -1 & \textcolor{red}{$\infty$} \\ \hline
			1  & 0  & 1 - 0 = 1  & $\infty$ \\ \hline
			1  & 1  & 1 - 1 = 0  & 0 \\ \hline
		\end{tabular}
	\end{table}
	\begin{block}{Entropia Cruzada}
		Para entrada $0, 1$ e saída predita $\textcolor{red}{0}$ e a saída desejada for $\textcolor{blue}{1}$
		\begin{gather*}
			L = - \frac{1}{N} \left [ \sum_{j=1}^{N} [\textcolor{red}{0} log(\textcolor{blue}{1} ) + (1-\textcolor{red}{0})log(1-\textcolor{blue}{1} )] \right ] 
		\end{gather*}
	\end{block}
\end{frame}
%==========================================================================================
\begin{frame}
	\frametitle{Neurônio Sigmoide}
	Vamos tomar:
	\begin{table}[]
	\begin{tabular}{|l|l|l|l|}
		\hline
		$p_j$ & $t_j$ & Erro  & $L$ \\ \hline
		0  & 0  & 0 - 0 = 0  & \textcolor{green}{0}  \\ \hline
		0  & 1  & 0 - 1 = -1 & \textcolor{red}{$\infty$} \\ \hline
		1  & 0  & 1 - 0 = 1  & \textcolor{red}{$\infty$} \\ \hline
		1  & 1  & 1 - 1 = 0  & 0 \\ \hline
	\end{tabular}
\end{table}
\begin{block}{Entropia Cruzada}
	Para entrada $1, 0$ e saída predita $\textcolor{red}{1}$ e a saída desejada for $\textcolor{blue}{0}$
	\begin{gather*}
		L = - \frac{1}{N} \left [ \sum_{j=1}^{N} [\textcolor{red}{1} log(\textcolor{blue}{0} ) + (1-\textcolor{red}{1})log(1-\textcolor{blue}{0} )] \right ] 
	\end{gather*}
\end{block}
\end{frame}
%==========================================================================================
\begin{frame}
	\frametitle{Neurônio Sigmoide}
	Vamos tomar:
	\begin{table}[]
		\begin{tabular}{|l|l|l|l|}
			\hline
			$p_j$ & $t_j$ & Erro  & $L$ \\ \hline
			0  & 0  & 0 - 0 = 0  & \textcolor{green}{0}  \\ \hline
			0  & 1  & 0 - 1 = -1 & \textcolor{red}{$\infty$} \\ \hline
			1  & 0  & 1 - 0 = 1  & \textcolor{red}{$\infty$} \\ \hline
			1  & 1  & 1 - 1 = 0  & \textcolor{green}{0} \\ \hline
		\end{tabular}
	\end{table}
	\begin{block}{Entropia Cruzada}
		Para entrada $1, 1$ e saída predita $\textcolor{red}{1}$ e a saída desejada for $\textcolor{blue}{1}$
		\begin{gather*}
			L = - \frac{1}{N} \left [ \sum_{j=1}^{N} [\textcolor{red}{1} log(\textcolor{blue}{1} ) + (1-\textcolor{red}{1})log(1-\textcolor{blue}{1} )] \right ] 
		\end{gather*}
	\end{block}
\end{frame}

%==========================================================================================
\begin{frame}
	\frametitle{Neurônio Sigmoide}
	\begin{block}{Vamos ver na prática}
		Vamos praticar utilizando o notebook 02\_neuronio\_sigmoid
	\end{block}
\end{frame}

%==========================================================================================
\section{Funções de Ativação}
\begin{frame}
	\frametitle{Funções de Ativação}
	\begin{itemize}
		\item Localizada a saída de cada neurônio
		\item Usada para mapear entradas em novas saídas
		\item Pode alterar o range ex: $[-100 \; 100]$ para $[1 \; 0]$
	\end{itemize}
	\begin{figure}
		\centering
		\includegraphics[width=0.4\linewidth]{figures/neuron_ai}
		\caption{Neurônio Artificial}
	\end{figure}
	
	\begin{gather*}
		\hat{y} = f( \sum_i w_i x_i + b)
	\end{gather*}
\end{frame}

%==========================================================================================
\begin{frame}
	\frametitle{Funções de Ativação}
	\begin{block}{Linear}
		\begin{itemize}
			\item $y \in [- \infty, + \infty]$
			\item Função de ativação simples
			\item Comumente usada em regressão
			\item Baixa complexidade
			\item Baixo poder de aprendizagem
		\end{itemize}
	Função:
			$$f(x) = x$$
	Derivada: 	$$\frac{\partial y}{\partial x} = 1$$
	
	\end{block}
\end{frame}

%==========================================================================================
\begin{frame}
	\frametitle{Funções de Ativação}
	\begin{alertblock}{Atenção}
		Note este caso: Porque construir modelos apenas com Lineares? \\
		input:	``10 → 100 → 200 → 10'' \\
		pesos:	``10 → 2 → 00.5 = 10 x 10 = 10'' \\
		Podemos substituir todos pesos por um só
	\end{alertblock}
	\begin{figure}
		\centering
		\includegraphics[width=0.4\linewidth]{figures/linear_function.png}
	\end{figure}
\end{frame}
%==========================================================================================
\begin{frame}
	\frametitle{Funções de Ativação}
	\begin{block}{Sigmoid}
		\begin{itemize}
			\item $y \in [0, + 1]$
			\item Regressão Logística
			\item Geralmente interpretada como probabilidade
			\item Saída não centrada em $0$
			\item Satura os gradientes
			\item Não indicada para camadas ocultas
			\item Converge lentamente
		\end{itemize}
		Função:
		$$f(x) = \frac{1}{1-e^{-x}}$$
		Derivada: 	$$\frac{\partial y}{\partial x} = y(1-y)$$
	\end{block}
\end{frame}
%==========================================================================================
\begin{frame}
	\frametitle{Funções de Ativação}
\begin{figure}
	\centering
	\includegraphics[width=0.4\linewidth]{figures/sigmoid_function.png}
\end{figure}
\end{frame}
%==========================================================================================
\begin{frame}
	\frametitle{Funções de Ativação}
	\begin{block}{Tanh}
		\begin{itemize}
			\item $y \in [-1, + 1]$
			\item Uma versão da Sigmoid
			\item Saída centrada em $0$
			\item Satura os gradientes. um pouco menos que a Sigmoid
			\item Converge lentamente
		\end{itemize}
		Função:
		$$f(x) = \frac{e^x - e^{-x}}{e^x+e^{-x}}$$
		Derivada: 	$$\frac{\partial y}{\partial x} = 1 - y^2$$
	\end{block}
\end{frame}
%==========================================================================================
\begin{frame}
	\frametitle{Funções de Ativação}
\begin{figure}
	\centering
	\includegraphics[width=0.4\linewidth]{figures/tanh_function}
\end{figure}
\end{frame}
%==========================================================================================
%==========================================================================================
\begin{frame}
	\frametitle{Funções de Ativação}
	\begin{block}{Relu}
		\begin{itemize}
			\item $y \in [0, + \infty]$
			\item Não tem derivada para valores $<0$
			\item Simples e Eficiente
			\item Evita a saturação dos gradientes
			\item Converge mais rápido
			\item Usada nas camadas escondidas
			\item Ela mata neurônio
		\end{itemize}
		Função:
		$$f(x) = max(0, x)$$
		Derivada: 	$$\frac{\partial y}{\partial x} =
		\left\{\begin{matrix}
			0,\; if \; x \leq 0
			\\ 
			1, \; if \; x > 0
		\end{matrix}\right.$$
	\end{block}
\end{frame}
%==========================================================================================
\begin{frame}
	\frametitle{Funções de Ativação}
	\begin{figure}
		\centering
		\includegraphics[width=0.4\linewidth]{figures/relu_function}
	\end{figure}
\end{frame}
%==========================================================================================
%==========================================================================================
\begin{frame}
	\frametitle{Funções de Ativação}
	\begin{block}{Leaky Relu}
		\begin{itemize}
			\item $y \in [- \infty, + \infty]$
			\item Tem derivada para valores $<0$
			\item Simples e Eficiente
			\item Evita a saturação dos gradientes
			\item Converge mais rápido
			\item Usada nas camadas escondidas
			\item Diminui as mortes de neurônios
		\end{itemize}
		Função:
		$$f(x) = 		\left\{\begin{matrix}
			\alpha(e^x - 1), x \leq 0
			\\ 
			x, x > 0
		\end{matrix}\right.$$
	
		Derivada: 	$$\frac{\partial y}{\partial x} =
		\left\{\begin{matrix}
			\alpha, x \leq 0
			\\ 
			1, x > 0
		\end{matrix}\right.$$
	\end{block}
\end{frame}
%==========================================================================================
\begin{frame}
	\frametitle{Funções de Ativação}
	\begin{figure}
		\centering
		\includegraphics[width=0.4\linewidth]{figures/leaky_relu_function}
	\end{figure}
	
\end{frame}
%==========================================================================================
\begin{frame}
	\frametitle{Funções de Ativação}
	\begin{figure}
		\centering
		\includegraphics[width=0.7\linewidth]{figures/mais_activation_functions}
	\end{figure}

\end{frame}
%===================================================================================
%==========================================================================================
\begin{frame}
	\frametitle{Funções de Ativação}
	\begin{alertblock}{ Qual função de ativação usar?}
		\begin{itemize}
		\item	Evitar Sigmoid nas camadas ocultas, boa na saída
		\item	Tanh usada em modelos generativos
		\item	Relu Muito boa nas camadas ocultas
		\item	Linear não usar em camadas escondidas
		\item	Leaky Relu  raramente usadas
		\end{itemize}
	\end{alertblock}	
\end{frame}
%===================================================================================
\begin{frame}
	\frametitle{Funções de Ativação}
	\begin{block}{Softmax}
		\begin{itemize}
			\item $y \in [0, 1]$ e $\sum_y = 1$
			\item Aplicada em dois ou mais neurônios. Pega saída e converte
			\item A saída é uma confiança
			\item Nunca nas camadas ocultas
			\item Multiclasses
			\item Saída One-hot Encode
		\end{itemize}
		Função:
		$$S_i = \frac{e^{\hat{y}_i}}{\sum_j e^{\hat{y}_i}} $$
		Assim para cada k: $P^k = S_i^{[k]}$
		Derivada: 	$$\frac{\partial S}{\partial y} = p^k * (1-p^k)$$
	\end{block}
\end{frame}
%==========================================================================================
\begin{frame}
	\frametitle{Funções de Ativação}
	\begin{block}{Softmax}
		\begin{example}
			$[0.1, 1.3, 2.5] \to [0.07, 0.22, 0.72] $ \\
			$$\frac{e^{0.1}}{e^{0.1} + e^{0.3} + e^{2.5}}$$
		\end{example}
		Função:
		$$S_i = \frac{e^{\hat{y}_i}}{\sum_j e^{\hat{y}_i}} $$
		Assim para cada k: $P^k = S_i^{[k]}$
		Derivada: 	$$\frac{\partial y}{\partial x} =
		\left\{\begin{matrix}
			\alpha, x \leq 0
			\\ 
			1, x > 0
		\end{matrix}\right.$$
	\end{block}
\end{frame}
%==========================================================================================
%==========================================================================================
\begin{frame}
	\frametitle{Funções de Ativação}
	\begin{block}{Vamos ver na prática}
		Vamos praticar utilizando o notebook 03\_funções de ativação
	\end{block}
\end{frame}

%==========================================================================================
\section{Backpropagation}
\begin{frame}
	\frametitle{Backpropagation}
	\begin{block}{Para que usamos o \textit{backpropagation}?}
		Utilizamos o Backpropagation no treinamento das redes neurais
	\end{block}
	\begin{example}
		Vamos tomar uma função simples de multiplicação:
		$$f(x, y) = x * y$$
			
		\begin{figure}
			\centering
			\includegraphics[width=0.5\linewidth]{figures/simple_multiply_gate}
		\end{figure}
	\end{example}
\end{frame}

%==========================================================================================
\begin{frame}
	\frametitle{Backpropagation}
	\begin{example}
		Vamos tomar uma função simples de multiplicação:
		$$f(x, y) = x * y$$
		
		\begin{figure}
			\centering
			\includegraphics[width=0.5\linewidth]{figures/simple_multiply_gate_part1}
		\end{figure}
		\alert{Com que força alterar as entradas desse circuito para de maneira ``leve'' alteremos a saída?}
	\end{example}
\end{frame}
%==========================================================================================
\begin{frame}
	\frametitle{Backpropagation}
	\begin{example}
		Vamos tomar uma função simples de multiplicação:
		$$f(x, y) = x * y$$
		
\begin{figure}
	\centering
	\includegraphics[width=0.5\linewidth]{figures/simple_multiply_gate_part2}
\end{figure}
		
	\end{example}
\end{frame}
%==========================================================================================
\begin{frame}
	\frametitle{Backpropagation}
	\begin{example}
		O Problema.....
		\begin{itemize}
			\item Para qual direção seguir?
			\item Com que velocidade seguir?
			\item Como sei se cheguei no local mais baixo?
		\end{itemize}
		
		\begin{figure}
			\centering
			\includegraphics[width=0.5\linewidth]{figures/alpinista.png}
		\end{figure}
		
	\end{example}
\end{frame}
%==========================================================================================
\begin{frame}
	\frametitle{Backpropagation}
	\begin{block}{Vamos ver na prática}
		Vamos praticar utilizando o notebook 03\_backpropagation
		Será apresentado a busca aleatória e a busca aleatória local
	\end{block}
\end{frame}
%==========================================================================================
\begin{frame}
	\frametitle{Backpropagation}
	\begin{block}{Como atualizamos os pesos?}
		Diante disso... \\
		É possível encontrar a força de atualização dos pesos com \alert{derivadas}
		
			\href{https://github.com/mafaldasalomao/pavic_treinamento_ml/blob/main/Machine_Learning/figures/random_01.gif?raw=true}{\beamergotobutton{Figura demonstrando}} 
	\end{block}
	\begin{block}{Qual a definição básica de derivada?}
		A derivada em relação à $x$ pode ser definida como:
		$$f'(x) = \lim_{h\rightarrow 0} \frac{f(x + h) - f (x))}{h}$$
		Neste caso, o $h$ tende a $0$, ou seja, é um valor bem pequeno.
	\end{block}
\end{frame}
%==========================================================================================
%==========================================================================================
\begin{frame}
	\frametitle{Backpropagation}
	\begin{block}{Qual a definição básica de derivada parcial?}
		E se tivéssemos $n$ argumentos (componentes)? \\
		A derivada parcial de uma função com $n$ argumentos $(x_1, x_2, x_3,...,x_n)$ é dada por:
		$$\frac{\partial f}{\partial x_i}(x_1,...,x_n) = \lim_{h \rightarrow 0} \frac{f(x_1, ...,x_i + h, x_n) - f (x_1, ..., x_n))}{h}$$
		Basta derivar cada elemento, ou cada componente da função.
	\end{block}
\begin{example}
	Derivada da função $f(x) = x^2$ \\
	$$f'(x) = \lim_{h \rightarrow 0} \frac{f(x + h)-f(x)}{h}$$
\end{example}
\end{frame}
%==========================================================================================
\begin{frame}
	\frametitle{Backpropagation}
	\begin{example}
		Derivada da função $f(x) = x^2$ \\
		$$f'(x) = \lim_{h \rightarrow 0} \frac{f(x + h)-f(x)}{h}$$
		$h \rightarrow 0$ tende a zero \\
		$f(x + h)$ é p próprio $f(x)$ somado com um pequeno passo \\
		$- f(x)$ que é a própria função \\
		Dividido por $h$ normalizando a derivação \\
		$$f'(x) = \lim_{h \rightarrow 0} \frac{f(x + h)-f(x)}{h}$$
		\renewcommand{\CancelColor}{\color{red}}
		$$\frac{(x+h)^2 - x^2}{h} \rightarrow \frac{\cancel{x^2}+ 2xh +h^2 - \cancel{ x^2}}{h} \rightarrow \frac{\cancel{h}(2x + h)}{\cancel{h}} \rightarrow 2x+\cancelto{0}{h}  \rightarrow 2x$$
	\end{example}
\end{frame}
%==========================================================================================
\begin{frame}
	\frametitle{Backpropagation}
	\begin{example}
		Derivada da função $f(x) = x*y$ \\
		$$f'(x) = \lim_{h \rightarrow 0} \frac{f(x + h)-f(x)}{h}$$
		\renewcommand{\CancelColor}{\color{red}}
		$$\frac{\partial f(x,y)}{\partial x} = \frac{(x+h)y - xy}{h} = \frac{\cancel{xy} + yh - \cancel{ xy}}{h} = \frac{y\cancel{h}}{\cancel{h}} = y$$
		$$\frac{\partial f(x,y)}{\partial y} = \frac{x(y+h) - xy}{h} = \frac{\cancel{xy} + xh - \cancel{ xy}}{h} = \frac{x\cancel{h}}{\cancel{h}} = x$$
	\end{example}
\end{frame}
%==========================================================================================
\begin{frame}
	\frametitle{Backpropagation}
	\begin{block}{Mais de duas variáveis na função...}
		E se a função tiver 3 componentes?
		$f(x,y,z) = (x+y)z$
		\begin{figure}
			\centering
			\includegraphics[width=0.7\linewidth]{figures/circuitxyz}
		\end{figure}
	Neste caso, a melhor opção é decompor a função em subfunções, ou pequenos circuitos
	\end{block}
\end{frame}
%==========================================================================================
\begin{frame}
	\frametitle{Backpropagation}
	\begin{block}{Mais de duas variáveis na função...}
		$f(x,y,z) = (x+y)z$ \\
		$q(x,y) = x+y$ \\
		$f(q,z) = qz$ \\
	\end{block}
\begin{example}
	\begin{columns}
		\begin{column}{0.35\textwidth}
			$$q(x,y) = x+y$$ \\
			$$\frac{\partial f(x,y)}{\partial x} = 1$$ \\
			$$\frac{\partial f(x,y)}{\partial y} = 1$$ \\
		\end{column}
		\begin{column}{0.35\textwidth}
			$$f(q,z) = qz$$ \\
			$$\frac{\partial f(x,y)}{\partial q} = z$$ \\
			$$\frac{\partial f(x,y)}{\partial z} = q$$ \\
		\end{column}
	\end{columns}
\end{example}
\end{frame}
%==========================================================================================
\begin{frame}
	\frametitle{Backpropagation}
	\begin{block}{Mais de duas variáveis na função...}
		Se quisermos calcular a derivada em relação a uma entrada, utilizamos a regra da cadeia: \\
		Ex: derivada em relação a $x$ de $f$
		\begin{itemize}
			\item É a derivada de $q$ em relação a $f$ 
			\item Derivada de $x$ em relação a $q$
		\end{itemize}
	\end{block}
	\begin{example}
				$$\frac{\partial f(x,y)}{\partial x} = \frac{\partial f(x,y)}{\partial q} * \frac{\partial f(x,y)}{\partial x}$$ \\
				$$\frac{\partial f(x,y)}{\partial y} = \frac{\partial f(x,y)}{\partial q} * \frac{\partial f(x,y)}{\partial y}$$ \\
				$$\frac{\partial f(x,y)}{\partial z} = q$$ 
	\end{example}
\end{frame}
%==========================================================================================
\begin{frame}
	\frametitle{Backpropagation}
	\begin{block}{Mais de duas variáveis na função...}
		Vamos verificar se dividindo a função principal em subfunções é válido.
	\end{block}
	\begin{example}
		\renewcommand{\CancelColor}{\color{red}}
		$\frac{\partial f(x,y, z)}{\partial x} = \lim_{h \rightarrow 0 }\frac{f(x+h, y,z) - f(x,y,z)}{h} = $ \\ 
		
		$ \frac{(x+h + y)*z - (x + y)*z}{h} = $ \\ $\frac{\cancel{xz}+hz + \cancel{yz} - \cancel{xz} - \cancel{yz}}{h} = $ \\ $\frac{\cancel{h}z}{\cancel{h}} = z $
	\end{example}
\end{frame}
%==========================================================================================
\begin{frame}
	\frametitle{Backpropagation}
	\begin{block}{Mais de duas variáveis na função...}
		Vamos verificar se dividindo a função principal em subfunções é válido.
	\end{block}
	\begin{example}
		\renewcommand{\CancelColor}{\color{red}}
		$\frac{\partial f(x,y, z)}{\partial y} = \lim_{h \rightarrow 0 }\frac{f(x, y+h,z) - f(x,y,z)}{h} = $ \\ 
		
		$ \frac{(x + y + h)*z - (x + y)*z}{h} = $ \\ $\frac{\cancel{xz}+hz + \cancel{yz} - \cancel{xz} - \cancel{yz}}{h} = $ \\ $\frac{\cancel{h}z}{\cancel{h}} = z $
	\end{example}
\end{frame}
%==========================================================================================
\begin{frame}
	\frametitle{Backpropagation}
	\begin{block}{Mais de duas variáveis na função...}
		Vamos verificar se dividindo a função principal em subfunções é válido.
	\end{block}
	\begin{example}
		\renewcommand{\CancelColor}{\color{red}}
		$\frac{\partial f(x,y, z)}{\partial z} = \lim_{h \rightarrow 0 }\frac{f(x, y,z+h) - f(x,y,z)}{h} = $ \\ 
		
		$ \frac{(x + y)*(z + h) - (x + y)*z}{h} = $ \\ $\frac{\cancel{xz}+xh + \cancel{yz} + yh - \cancel{xz} - \cancel{yz}}{h} = $ \\ $\frac{xh + yh}{h} =  $\\ $\frac{\cancel{h}(x + y)}{\cancel{h}} = x + y $
		
		Podemos notar que a saída é a porta de soma, ou seja. o próprio $q$
	\end{example}
\end{frame}
%==========================================================================================
\begin{frame}
	\frametitle{Backpropagation}
	\begin{block}{Vamos derivar o Neurônio Sigmoide}
		\begin{itemize}
			\item $y \in [0, + 1]$
			\item Regressão Logística
			\item Geralmente interpretada como probabilidade
			\item Saída não centrada em $0$
			\item Satura os gradientes
			\item Não indicada para camadas ocultas
			\item Converge lentamente
		\end{itemize}
		Função:
		$$f(x) = \frac{1}{1-e^{-x}}$$
	%	Derivada: 	$$\frac{\partial y}{\partial x} = y(1-y)$$
	\end{block}
\end{frame}
%==========================================================================================
\begin{frame}
	\frametitle{Backpropagation}
	\begin{block}{Vamos derivar o Neurônio Sigmoide}
		\begin{itemize}
			\item $y \in [0, + 1]$
			\item Regressão Logística
			\item Geralmente interpretada como probabilidade
			\item Saída não centrada em $0$
			\item Satura os gradientes
			\item Não indicada para camadas ocultas
			\item Converge lentamente
		\end{itemize}
		Função:
		$$f(x) = \frac{1}{1-e^{-x}}$$
		%	Derivada: 	$$\frac{\partial y}{\partial x} = y(1-y)$$
	\end{block}
\end{frame}
%==========================================================================================
\begin{frame}
	\frametitle{Backpropagation}
	\begin{block}{Vamos derivar o Neurônio Sigmoide}
		Função:
		$f(w, x) = \frac{1}{1-e^{-x}}$ \\ 
		O $w$ são os pesos e o $x$ as entradas... \\
		$f(w, x) = \frac{1}{1-e^{-(w_0x_0 + w_1x_1 + w_2)}}$
		%	Derivada: 	$$\frac{\partial y}{\partial x} = y(1-y)$$
		\begin{figure}
			\centering
			\includegraphics[width=0.7\linewidth]{figures/sigmoidneuron_derivative}
		\end{figure}
		
	\end{block}
\end{frame}
%==========================================================================================
\begin{frame}
	\frametitle{Backpropagation}
	\begin{example}
		Vamos praticar um pouquinho... \\
		A derivada de $x = a * b$ é: \\
		$da = b * dx$ \\
		Sempre teremos a derivada de dentro \alert{$da $} multiplicando a derivada de quem está fora \alert{$dx$}. Desta forma, podemos decompor qualquer função complexa. E $db = a * dx$
		\begin{columns}
			\begin{column}{0.35\textwidth}
				 $$x = a * b$$ 
				$$da = b * dx$$ 
				$$db = a * dx$$ 
			\end{column}
			\begin{column}{0.35\textwidth}
				$$x = a + b$$ 
				$$da = 1 * dx$$ 
				$$db = 1 * dx$$ 
			\end{column}
		\end{columns}
	\end{example}
\end{frame}
%==========================================================================================
\begin{frame}
	\frametitle{Backpropagation}
	\begin{example}
		Vamos ver a derivada da função $x = a + b + c$ \\
		Primeiro vamos dividir por partes: 
		$q = a+b$ e  $x = q+c$ 
		\begin{columns}
			\begin{column}{0.35\textwidth}
				$$dq = 1 * dx$$
				$$dc = 1 * dx$$
				$$db = 1 * dq$$
				$$da = 1 * dq$$
			\end{column}
			\begin{column}{0.35\textwidth}
				$$x = a + b + c$$ 
				$$da = 1 * dx$$ 
				$$db = 1 * dx$$
				$$dc = 1 * dx$$ 
			\end{column}
		\end{columns}
		Intuitivamente pode-se perceber que a derivação da soma sempre será $1 * dx$
	\end{example}
\end{frame}
%==========================================================================================
\begin{frame}
	\frametitle{Backpropagation}

		\begin{columns}
			\begin{column}{0.35\textwidth}
				\begin{example}
					$$x = a * b + c$$
					$$q = a*b dx$$ 
					$$x = q+c$$ 
					$$dq = 1 * dx$$
					$$dc = 1 * dx$$
					$$db = a * dq$$
					$$da = b * dq$$
				\end{example}
			\end{column}
			\begin{column}{0.35\textwidth}
				\begin{example}
					$$x = a * a$$ 
					$$da = 2a * dx$$
				\end{example} 
				\begin{example}
				$$x = a * a + b * b + c * c$$ 
				$$da = 2a * dx$$
				$$db = 2b * dx$$
				$$dc = 2c * dx$$
			\end{example} 
			\end{column}
		\end{columns}

\end{frame}
%==========================================================================================
\begin{frame}
	\frametitle{Backpropagation}
	\begin{example}
		Vamos ver este exemplo mais complexo
		\begin{columns}
			\begin{column}{0.35\textwidth}
				$$x = ((a * b + c) * d)^2$$
				$$x_1 = a*b+c$$ 
				$$x_2 = x_1*d$$ 
				$$x = x_2 * x_2 $$
			\end{column}
			\begin{column}{0.35\textwidth}
				$$dx_2 = 2x_2 * dx$$ 
				$$dx_1 = d * dx_2$$ 
				$$dd = x_1*dx_2$$
				$$da = b*dx_1$$
				$$db = a*dx_1$$
				$$dc = 1*dx_1$$
			\end{column}
		\end{columns}
	\end{example}
\end{frame}
%==========================================================================================
\begin{frame}
	\frametitle{Backpropagation}
		\begin{example}
			Vamos ver este exemplo de divisão
			$$x = \frac{1}{a}$$
			$$da = (-\frac{1}{a*a})*dx_1$$
		\end{example}
\end{frame}
%==========================================================================================
\begin{frame}
	\frametitle{Backpropagation}
	\begin{example}
	\begin{columns}
		\begin{column}{0.35\textwidth}
			
				$$x = \frac{a+ b}{c +d}$$
				$$x_1 = a + b$$
				$$x_2 = c + d$$
				$$x_3 = \frac{1}{x_2} $$
				$$x = x_1 * x_3$$
		\end{column}
		\begin{column}{0.35\textwidth}
			$$dx_1 = x_3 * dx$$
			$$dx_3 = x_1 * dx$$
			$$dx_2 = - \frac{1}{x_2 * x_2}*dx_3$$
			$$dc = 1 * dx_2$$
			$$dd = 1 * dx_2$$
			$$da = 1 * dx_1$$
			$$db = 1 * dx_1$$
		\end{column}
	\end{columns}
\end{example}
\end{frame}
%==========================================================================================
\begin{frame}
	\frametitle{Backpropagation}
	\begin{example}
		$$x = max(a, b)$$
		$$da = x == a ? 1 * dx : 0$$
		$$da = x == b ? 1 * dx : 0$$
		Lembram da Relu?
		$$x = max(0, a)$$
		$$da = a> 0 ? 1 * dx : 0$$
	\end{example}
\end{frame}
%==========================================================================================
\begin{frame}
	\frametitle{Backpropagation}
	\begin{block}{Como fazer derivação de matrizes}
		O que vimos até agora foram números escalares... \\
		Como derivar uma matriz?
	\end{block}
	\begin{example}	
		
		$W = np.random.randn(5, 10)$ \\
		$X = np.random.randn(3, 10)$ \\
		$Y = X.dot(W^T)$ \\
		$W$ são nossos pesos, $X$ nossas entradas, $Y$ a saída \\
		$10$ é a dimensão da entrada, $3$ Qtd de amostras, $5$ Qtd de neurônios. \\
		Anteriormente só tínhamos 1 neurônio \\ 
		A multiplicação de uma matriz de $3x10 * 10x5$ gerará uma saída $3x5$
	\end{example}
\end{frame}
%==========================================================================================
\begin{frame}
	\frametitle{Backpropagation}
	\begin{example}	
		Vamos imaginar que nosso $Y = WX$
		Se a multiplicação de uma matriz de $3x10 * 10x5$ gerará uma saída $3x5$ a derivação de $dY$ deverá ter a dimensão de $3x5$. Assim, a derivada de uma matriz sempre terá o shape da matriz original \\
		$dY = np.random.randn(*Y.shape)$ \\
		A derivada de $W$ dado $Y = WX$ será o de dentro * o de fora $X * dY$, assim: \\
		$dW = dY^T.dot(X)$\\
		A derivada de $X$ dado $Y = WX$ será o de dentro * o de fora $W * dY$, assim: \\
		$dY = dY.dot(W)$
		
		\alert{A derivada de uma matriz deverá possuir o mesmo \textit{shape} da matriz original, neste caso $Y$ tem $3x5$ e $dY$ também}
	\end{example}
\end{frame}
%==========================================================================================
\begin{frame}
	\frametitle{Backpropagation}
	\begin{block}{Resumo das derivadas}
		Vamos tomar este exemplo:
		\begin{figure}
			\centering
			\includegraphics[width=0.7\linewidth]{figures/simple_circuit}
		\end{figure}
	\end{block}
\end{frame}
%==========================================================================================
\begin{frame}
	\frametitle{Backpropagation}
	\begin{block}{Resumo das derivadas}
		Vamos tomar este exemplo:
		\begin{figure}
			\centering
			\includegraphics[width=0.7\linewidth]{figures/simple_circuit2}
		\end{figure}
	\end{block}
\end{frame}
%==========================================================================================
\begin{frame}
	\frametitle{Backpropagation}
	\begin{block}{Derivando o neurônio Sigmoide}
		Agora que sabemos como derivar, vamos retornar ao neurônio Sigmoide:
		\begin{figure}
			\centering
			\includegraphics[width=1\linewidth]{figures/sigmoidneuron_derivative}
		\end{figure}
	\end{block}
\end{frame}
%==========================================================================================
%==========================================================================================
\begin{frame}
	\frametitle{Backpropagation}
	\begin{block}{Derivando o neurônio Sigmoide}
		Vamos fazer o processo de \textit{forward}.
		\begin{figure}
			\centering
			\includegraphics[width=1\linewidth]{figures/sigmoid_example}
		\end{figure}
	\end{block}
\end{frame}
%==========================================================================================
\begin{frame}
	\frametitle{Backpropagation}
	\begin{block}{Derivando o neurônio Sigmoide}
		Após o \textit{forward}, faremos agora a propagação reversa ou \textit{backpropagation}.
		\begin{figure}
			\centering
			\includegraphics[width=1\linewidth]{figures/sigmoid_example_backprop}
		\end{figure}
	\end{block}
\end{frame}
%==========================================================================================
\begin{frame}
	\frametitle{Funções de Ativação}
	\begin{block}{Vamos ver na prática}
		Vamos praticar utilizando o notebook 04\_backpropagation
	\end{block}
\end{frame}
%==========================================================================================




\section{Redes Neurais Profundas}

\begin{frame}
	\frametitle{Redes Neurais Profundas}
	\begin{block}{Dimensão das matrizes}
		Até o momento vimos que um neurônio pode ser dado pela equação: $y = f(xw^T + b)$. Sendo $f$ uma função de ativação. A função de ativação não altera o \textit{shape} dos dados. \\
		A princípio tivemos $x = [1x D_{in}]$ \textit{shape}. Onde $1$ era a quantidade de amostras e $D_{in}$ o dimensões da amostra, por exemplo, a porta OR que recebe duas entradas $(x_1, x_2)$, assim $D_{in} = 2$. \\
		O mesmo ocorre para a saída, $y = [1x D_{out}]$ \textit{shape}. Onde $1$ era a quantidade de amostras e $D_{out}$ a dimensão de saída, em todos casos visto até agora igual a $1$. Mas podemos ter quantas saídas forem necessárias. \\
		Mas qual a dimensão do \textit{bias} e dos nossos pesos?
				
	\end{block}
\end{frame}

%==========================================================================================
\begin{frame}
	\frametitle{Redes Neurais Profundas}
	\begin{block}{Dimensão das matrizes}
		Assim...
		\begin{itemize}
			\item $x = [1x D_{in}]$
			\item $y = [1x D_{out}]$
			\item $bias = [1x D_{out}]$
			\item $w^T = [D_{in} x D_{out}]$
			\item $[1x D_{out}] = [1 x D_{in}] * [D_{in} x D_{out}] + [1x D_{out}]$
		\end{itemize}
	\end{block}
\end{frame}
%==========================================================================================
\begin{frame}
	\frametitle{Redes Neurais Profundas}
	\begin{block}{Dimensão das matrizes}
		\begin{columns}
			\begin{column}{0.4\textwidth}
				\begin{figure}
					\centering
					\includegraphics[width=1\linewidth]{figures/simple_nn1}
				\end{figure}
			\end{column}
			\begin{column}{0.6\textwidth}
				\begin{table}[]
					\begin{tabular}{|l|l|l|l|}
						\hline
						$[1 x D_{in}]$ & $[D_{in} x D_{out}]$ &  $[1x D_{out}]$ & $ [1x D_{out}]$ \\ \hline
						$1X3$ & $3x4$ &  $1x4$ & $1x4$  \\ \hline
					\end{tabular}
				\end{table}
			A multiplicação das matrizes $1X3 * 3x4$ nos retornará uma matriz $1x4$, que deve ser a mesma dimensão do \textit{bias}. \\
			Cada neurônio possui um bias.
			\end{column}
		\end{columns}

	\end{block}
\end{frame}
%==========================================================================================
\begin{frame}
	\frametitle{Redes Neurais Profundas}
	\begin{block}{Dimensão das matrizes}
		\begin{columns}
			\begin{column}{0.4\textwidth}
				\begin{figure}
					\centering
					\includegraphics[width=1\linewidth]{figures/simple_nn2}
				\end{figure}
			\end{column}
			\begin{column}{0.6\textwidth}
				\begin{table}[]
					\begin{tabular}{|l|l|l|l|}
						\hline
						$[1 x D_{in}]$ & $[D_{in} x D_{out}]$ &  $[1x D_{out}]$ & $ [1x D_{out}]$ \\ \hline
						$1X3$ & $3x4$ &  $1x4$ & $1x4$  \\ \hline
						$1X4$ & $4x4$ &  $1x4$ & $1x4$  \\ \hline
					\end{tabular}
				\end{table}
				A multiplicação das matrizes $1X3 * 4x4$ nos retornará uma matriz $1x4$, que deve ser a mesma dimensão do \textit{bias}. \\
				Cada neurônio possui um bias.
			\end{column}
		\end{columns}
		
	\end{block}
\end{frame}
%==========================================================================================
\begin{frame}
	\frametitle{Redes Neurais Profundas}
	\begin{block}{Dimensão das matrizes}
		\begin{columns}
			\begin{column}{0.4\textwidth}
				\begin{figure}
					\centering
					\includegraphics[width=1\linewidth]{figures/simple_nn3}
				\end{figure}
			\end{column}
			\begin{column}{0.6\textwidth}
				\begin{table}[]
					\begin{tabular}{|l|l|l|l|}
						\hline
						$[1 x D_{in}]$ & $[D_{in} x D_{out}]$ &  $[1x D_{out}]$ & $ [1x D_{out}]$ \\ \hline
						$1X3$ & $3x4$ &  $1x4$ & $1x4$  \\ \hline
						$1X4$ & $4x4$ &  $1x4$ & $1x4$  \\ \hline
						$1X4$ & $4x1$ &  $1x1$ & $1x1$  \\ \hline
					\end{tabular}
				\end{table}
				A multiplicação das matrizes $1X1 * 4x1$ nos retornará uma matriz $1x1$, que deve ser a mesma dimensão do \textit{bias}.
			\end{column}
		\end{columns}
		
	\end{block}
\end{frame}
%==========================================================================================
\begin{frame}
	\frametitle{Redes Neurais Profundas}
	\begin{block}{Dimensão das matrizes}
		\begin{columns}
			\begin{column}{0.4\textwidth}
				\begin{figure}
					\centering
					\includegraphics[width=1\linewidth]{figures/simple_nn}
				\end{figure}
			\end{column}
			\begin{column}{0.6\textwidth}
				Quantos parâmetros treináveis esta rede possui? \\
				\begin{table}[]
					\begin{tabular}{|l|l|l|l|}
						\hline
						$[1 x D_{in}]$ & $[D_{in} x D_{out}]$ &  $[1x D_{out}]$ & $ [1x D_{out}]$ \\ \hline
						$1X3$ & $3x4=\alert{12}$ &  $1x4=\alert{4}$ & $1x4$  \\ \hline
						$1X4$ & $4x4=\alert{16}$ &  $1x4=\alert{4}$ & $1x4$  \\ \hline
						$1X4$ & $4x1=\alert{4}$ &  $1x1=\alert{1}$ & $1x1$  \\ \hline
					\end{tabular}
				\end{table}
				$12 + 4 + 16 + 4 + 4 + 1 = 41$ Parâmetros
			\end{column}
		\end{columns}
	\end{block}
\end{frame}
%==========================================================================================


\section{Funções de Custo}

\begin{frame}
	\frametitle{Funções de Custo - Regressão}
		\begin{columns}
		\begin{column}{0.4\textwidth}
			\begin{block}{MAE- mean of absolute errors}
				$$J = \frac{1}{N} \sum |y_i - \hat{y}_i|$$ \\
				$$\frac{\partial J}{\partial \hat{y}} = \frac{1}{N} \left\{\begin{matrix}
					+1 se \hat{y} > y
					\\ 
					-1 se \hat{y} < y
				\end{matrix}\right.$$ 
			\end{block}
		\end{column}
		\begin{column}{0.4\textwidth}
			\begin{block}{MSE- mean of squared errors}
				$$J = \frac{1}{2N} \sum |y_i - \hat{y}_i|^2$$ \\
				$$\frac{\partial J}{\partial \hat{y}} = -(y-\hat{y})\frac{1}{N}$$ 
			\end{block}
		\end{column}
	\end{columns}
\end{frame}
%==========================================================================================

\begin{frame}
	\frametitle{Funções de Custo - Classificação binária}
	\begin{block}{Binary Cross Entropy}
		$$J = -\frac{1}{N} \sum y_iln(\hat{y}_i)+(1-y_i)ln(1-\hat{y}_i)$$ \\
		$$\frac{\partial J}{\partial \hat{y}} = \frac{-(y-\hat{y})}{\hat{y}(1-\hat{y})}\frac{1}{N}$$ 
	\end{block}
\end{frame}

%==========================================================================================

\begin{frame}
	\frametitle{One hot Encode}
	\begin{block}{One hot Encode}
		Antes de falar sobre classificação de classes, vamos entender a seguinte situação:
		\begin{columns}
			\begin{column}{0.4 \textwidth}
				\begin{table}[]
					\begin{tabular}{|l|l|}
						\hline
						y & One-hot \\ \hline
						1 & 1 0 0 0 \\ \hline
						2 & 0 1 0 0 \\ \hline
						3 & 0 0 1 0 \\ \hline
						4 & 0 0 0 1 \\ \hline
					\end{tabular}
				\end{table}
			\end{column}
			\begin{column}{0.4 \textwidth}
				\begin{table}[]
					\begin{tabular}{|l|l|}
						\hline
						y & One-hot \\ \hline
						3 & 0 0 1 0 \\ \hline
						2 & 0 1 0 0 \\ \hline
						4 & 0 0 0 1 \\ \hline
						1 & 1 0 0 0 \\ \hline
					\end{tabular}
				\end{table}
			\end{column}
		\end{columns}


	\end{block}
\end{frame}

%==========================================================================================
\begin{frame}
	\frametitle{Funções de Custo - Classificação Multiclasse}
	\begin{block}{Softmax}
		Função:
		$$S_i = \frac{e^{\hat{y}_i}}{\sum_j e^{\hat{y}_i}} $$
		Assim para cada k: $P^k = S_i^{[k]}$
		Derivada: 	$$\frac{\partial S}{\partial y} = p^k * (1-p^k)$$
	\end{block}
	\begin{alertblock}{Atenção}
		A Softmax não é uma função de custo.
	\end{alertblock}
\end{frame}
%==========================================================================================
%==========================================================================================
\begin{frame}
	\frametitle{Funções de Custo - Classificação Multiclasse}
	
	\begin{columns}
		\begin{column}{0.4 \textwidth}
		\begin{block}{Softmax}
			$$S_i = \frac{e^{\hat{y}_i}}{\sum_j e^{\hat{y}_i}} $$
			Assim para cada k: $P^k = S_i^{[k]}$
			Derivada: 	$$\frac{\partial S}{\partial y} = p^k * (1-p^k)$$
		\end{block}
		\end{column}
		\begin{column}{0.4 \textwidth}
		\begin{block}{Neg. Log-likelihood}
			$$J = \frac{1}{N} \sum -ln(p_i^k) $$
			 $$\frac{\partial J}{\partial p^k} = - \frac{1}{p^k}$$
			Derivada: 	$$\frac{\partial J}{\partial \hat{y}} = -(1-p^k) = -(y-\hat{y}) \frac{1}{N}$$
		\end{block}
		\end{column}
	\end{columns}
	\begin{alertblock}{Atenção}
		A Neg. Log-likelihood será o somatório do logarítmico  da negação para cada elemento da Softmax pelo total de elementos.
	\end{alertblock}
\end{frame}
%==========================================================================================
\begin{frame}
	\frametitle{Funções de Custo}
	\begin{block}{Qual função de custo utilizar?}
	\begin{table}[]
		\begin{tabular}{c|ccc|}
			\cline{2-4}
			& \multicolumn{3}{c|}{Problema}                                             \\ \hline
			\multicolumn{1}{|c|}{} &
			\multicolumn{1}{c|}{Regressão} &
			\multicolumn{1}{c|}{\begin{tabular}[c]{@{}c@{}}Classificação\\ Binária\end{tabular}} &
			\begin{tabular}[c]{@{}c@{}}Classificação\\ Multiclasse\end{tabular} \\ \hline
			\multicolumn{1}{|c|}{\begin{tabular}[c]{@{}c@{}}\#neurônios\\ ult. camada\end{tabular}} & \multicolumn{1}{c|}{\#outputs} & \multicolumn{1}{c|}{1}       & \#classes \\ \hline
			\multicolumn{1}{|c|}{\begin{tabular}[c]{@{}c@{}}F. Ativação\\ ult. camada\end{tabular}} & \multicolumn{1}{c|}{Linear}    & \multicolumn{1}{c|}{Sigmoid} & Linear    \\ \hline
			\multicolumn{1}{|c|}{F. de Custo} &
			\multicolumn{1}{c|}{\begin{tabular}[c]{@{}c@{}}MSE, MAE,\\ SSE, ...\end{tabular}} &
			\multicolumn{1}{c|}{Cross Entropy} &
			\begin{tabular}[c]{@{}c@{}}Softmax +\\ Neg. Log-Likelihood\end{tabular} \\ \hline
		\end{tabular}
	\end{table}
	\end{block}
\end{frame}

%==========================================================================================

%==========================================================================================
\begin{frame}
	\frametitle{Funções de Ativação}
	\begin{block}{Vamos ver na prática}
		Vamos praticar utilizando o notebook 05\_redesneuraisintuicao
	\end{block}
\end{frame}
%==========================================================================================

\section{Rede Neural do Zero}

\begin{frame}
	\frametitle{Funções de Custo - Regressão}
	\begin{block}{Passo 1}
		Primeiramente vamos implementar nossas funções de custo... \\
		Será Utilizado o notebook 06\_rede neural
	\end{block}
	\begin{columns}
		\begin{column}{0.4\textwidth}
			\begin{block}{MAE- mean of absolute errors}
				$$J = \frac{1}{N} \sum |y_i - \hat{y}_i|$$ \\
				$$\frac{\partial J}{\partial \hat{y}} = \frac{1}{N} \left\{\begin{matrix}
					+1 se \hat{y} > y
					\\ 
					-1 se \hat{y} < y
				\end{matrix}\right.$$ 
			\end{block}
		\end{column}
		\begin{column}{0.4\textwidth}
			\begin{block}{MSE- mean of squared errors}
				$$J = \frac{1}{2N} \sum |y_i - \hat{y}_i|^2$$ \\
				$$\frac{\partial J}{\partial \hat{y}} = -(y-\hat{y})\frac{1}{N}$$ 
			\end{block}
		\end{column}
	\end{columns}
\end{frame}
%==========================================================================================
\begin{frame}
	\frametitle{Funções de Custo - Classificação binária}
	\begin{block}{Passo 2}
		Vamos implementar nossas funções de custo... \\
	\end{block}
	\begin{block}{Binary Cross Entropy}
		$$J = -\frac{1}{N} \sum y_iln(\hat{y}_i)+(1-y_i)ln(1-\hat{y}_i)$$ \\
		$$\frac{\partial J}{\partial \hat{y}} = \frac{-(y-\hat{y})}{\hat{y}(1-\hat{y})}\frac{1}{N}$$ 
	\end{block}
\end{frame}
%==========================================================================================




\begin{frame}
	
\frametitle{Highlighting text}
%
%\begin{align}
%	a + b  q q= c \\        
%	a = c - b
%\end{align}
In this slide, some important text will be
\alert{highlighted} because it's important.
Please, don't abuse it.

\begin{block}{Remark}
Sample text
\end{block}

\begin{alertblock}{Important theorem}
Sample text in red box
\end{alertblock}

\begin{examples}
Sample text in green box. The title of the block is ``Examples".
\end{examples}
\end{frame}

\end{document}