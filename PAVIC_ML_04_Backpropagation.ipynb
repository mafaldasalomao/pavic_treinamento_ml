{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyPXrtpyNDMhET40y9Am8xS3",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mafaldasalomao/pavic_treinamento_ml/blob/main/PAVIC_ML_04_Backpropagation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "%matplotlib inline"
      ],
      "metadata": {
        "id": "xSaoX9nyOAUq"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "A melhor maneira de pensar em redes neurais é como circuitos de valores reais. Mas, ao invés de valores booleanos, valores reais e, ao invés de portas lógicas como **and** ou **or**, portas binárias (dois operandos) como $*$ (multiplicação), + (adição), max, exp, etc. Além disso, também teremos **gradientes** fluindo pelo circuito, mas na direção oposta."
      ],
      "metadata": {
        "id": "id9SLPA__hOq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def forwardMultiplyGate(a, b):\n",
        "  return a * b"
      ],
      "metadata": {
        "id": "Y0f-wxW8_FMQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "De forma matemática, a gente pode considerar que essa porta implementa a seguinte função:\n",
        "\n",
        "$$f(x,y)=x*y$$"
      ],
      "metadata": {
        "id": "4XcXrk9o_l9C"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## O Objetivo"
      ],
      "metadata": {
        "id": "6wgw17iG_rO9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Vamos imaginar que temos o seguinte problema:\n",
        "1. Nós vamos providenciar a um circuito valores específicos como entrada (x=1, y=-3)\n",
        "2. O circuito vai calcular o valor de saída (-3)\n",
        "3. A questão é: *Quanto mudar a entrada para levemente **aumentar** a saída?*\n",
        "\n",
        "No nosso caso, em que direção devemos mudar x,y para conseguir um número maior que -3? Note que, pro nosso exemplo, se x = 0.99 e y = -2.99, x$*$y = -2.96 que é maior que -3. **-2.96 é melhor (maior) que -3**, e obtivemos uma melhora de 0.04."
      ],
      "metadata": {
        "id": "qcK3YjMG_std"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Estratégia 1: Busca Aleatória"
      ],
      "metadata": {
        "id": "RM1sM-VTALO5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ok. Isso não é trivial? A gente pode simplesmente gerar valores aleatórios, calcular a saída e guardar o melhor resultado."
      ],
      "metadata": {
        "id": "jRvXHkJOAMBW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x, y = 1, -3\n",
        "melhor_saida = forwardMultiplyGate(x,y)\n",
        "melhor_x, melhor_y = 0, 0\n",
        "\n",
        "for k in range(0,100):\n",
        "    x_try = 5*np.random.random() - 5\n",
        "    y_try = 5*np.random.random() - 5\n",
        "    out = forwardMultiplyGate(x_try, y_try)\n",
        "    \n",
        "    if out > melhor_saida:\n",
        "        melhor_saida = out\n",
        "        melhor_x, melhor_y = x_try, y_try\n",
        "\n",
        "print(melhor_x, melhor_y, forwardMultiplyGate(melhor_x, melhor_y))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pPTRi7k3_sMO",
        "outputId": "60fca1bd-03a6-43ca-e2fc-79a55cc846c0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-4.951508016242007 -4.8342603703596305 23.936878976436766\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ok, foi bem melhor. Mas, e se tivermos milhões de entradas? É claro que essa estratégia não funcionará. Vamos tentar algo mais aprimorado."
      ],
      "metadata": {
        "id": "diiwswcmAW4d"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Estratégia 2: Busca Aleatória Local"
      ],
      "metadata": {
        "id": "1nP7MgznAemH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Um passo aleatorio em qualquer direçao, torcer para tomar a decisao correta\n",
        "agora temos um passo pra evitar um pulo muito longo\n"
      ],
      "metadata": {
        "id": "zy5QhY0nAfIw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x, y = 1, -3\n",
        "passo = 0.01\n",
        "melhor_saida = forwardMultiplyGate(x,y)\n",
        "melhor_x, melhor_y = 0, 0\n",
        "\n",
        "for k in range(0,100):\n",
        "    x_try = x + passo * (2*np.random.random() - 1)\n",
        "    y_try = y + passo * (2*np.random.random() - 1)  \n",
        "    out = forwardMultiplyGate(x_try, y_try)\n",
        "    \n",
        "    if out > melhor_saida:\n",
        "        melhor_saida = out\n",
        "        melhor_x, melhor_y = x_try, y_try\n",
        "\n",
        "print(melhor_x, melhor_y, forwardMultiplyGate(melhor_x, melhor_y))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wqIqVaLK_Zn1",
        "outputId": "9b8ccc6a-eb80-4432-c146-223fd44649bd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.9914489101662263 -2.99150670342498 -2.9659260608656566\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Otimoooo! Demos um passinho mais controlado -2.96 é menor que -3.... mas precisamos de algo melhor, uma estratégia mais inteligente."
      ],
      "metadata": {
        "id": "9MbM8KF0BN6T"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Estratégia 3: Gradiente Numérico"
      ],
      "metadata": {
        "id": "6gGRcmPiBZOC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Imagine agora que a gente pega as entradas de um circuito e puxa-as para uma direção positiva. Essa força puxando $x$ e $y$ vai nos dizer como $x$ e $y$ devem mudar para aumentar a saída. Não entendeu? Vamos explicar:\n",
        "\n",
        "Se olharmos para as entradas, a gente pode intuitivamente ver que a força em $y$ deveria sempre ser positiva, porque tornando $y$ um pouquinho maior de $y=1$ para $y=-1$ aumenta a saída do circuito para $-1$, o que é bem maior que $-3$. Por outro lado, se a força em $x$ for negativa, tornando-o menor, como de $x=1$ para $x=0.5$, também aumenta a saída: $-0.5\\times-3 = -1.5$, de novo maior que $-3$.\n",
        "\n",
        "E como calcular essa força? Usando **derivadas**.\n",
        "\n",
        "> *A derivada pode ser pensada como a força que a gente aplica em cada entrada para aumentar a saída*\n",
        "\n",
        "\n",
        "E como exatamente a gente vai fazer isso? Em vez de olhar para o valor de saída, como fizemos anteriormente, nós vamos iterar sobre as cada entrada individualmente, aumentando-as bem devagar e vendo o que acontece com a saída. **A quantidade que a saída muda é a resposta da derivada**.\n",
        "\n",
        "Vamos para definição matemática. A derivada em relação a $x$ pode ser definida como:\n",
        "\n",
        "$$\\frac{\\partial f(x,y)}{\\partial x} = \\frac{f(x+h,y) - f(x,y)}{h}$$\n",
        "\n",
        "Onde $h$ é pequeno. Nós vamos, então, calcular a saída inicial $f(x,y)$ e aumentar $x$ por um valor pequeno $h$ e calcular a nova saída $f(x+h,y)$. Então, nós subtraimos esse valores para ver a diferença e dividimos por $f(x+h,y)$ para normalizar essa mudança pelo valor (arbitrário) que nós usamos.\n",
        "\n",
        "Em termos de código, teremos:"
      ],
      "metadata": {
        "id": "_i9gKYaxBZ14"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x, y = 1, -3\n",
        "out = forwardMultiplyGate(x,y) # f(x,y)\n",
        "h = 0.01 # o h tende a 0\n",
        "\n",
        "# derivada em relação a x\n",
        "out2 = forwardMultiplyGate(x+h, y) # f(x + h, y)\n",
        "\n",
        "derivada_x = (out2- out)/h\n",
        "\n",
        "# derivada em relação a y\n",
        "out3 = forwardMultiplyGate(x, y+h) # f(x + h, y)\n",
        "\n",
        "derivada_y = (out3 - out)/h\n",
        "print(out2, out3)\n",
        "derivada_x, derivada_y "
      ],
      "metadata": {
        "id": "e1IBXIQ1BM4V",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ba6697c9-9d89-4b85-af40-ea5ef0674067"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-3.0300000000000002 -2.99\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(-3.000000000000025, 0.9999999999999787)"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Essas saídas representam a foça da atualização naquele ponto\n",
        "o *3* indica que a força da derivada de $x$ é maior que a força da derivada de $y$\n",
        "O sinal indica somente se a função cresce ou decresce\n",
        "\n",
        "> *A derivada em relação a alguma entrada pode ser calculada ajustando levemente aquela entrada e observando a mudança no valor da saída*\n",
        "\n",
        "A derivada é calculada sobre cada entrada, enquanto o **gradiente** representa todas as derivadas sobre as entradas concatenadas em um vetor.\n",
        "> Derivada é so um valor e o gradiente eh o vertor desses valores"
      ],
      "metadata": {
        "id": "SVHuVGboa2dO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x, y = 1, -3\n",
        "passo = 0.001\n",
        "out = forwardMultiplyGate(x, y) # f(x , y)\n",
        "x = x + passo * derivada_x # atualizamos os pesos\n",
        "y = y + passo * derivada_y\n",
        "out_new = forwardMultiplyGate(x, y)\n",
        "\n",
        "print(out_new)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dZAQUVl4ZaSE",
        "outputId": "8a4c7c10-1055-47f4-8e3d-1ebd77225ad5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-2.990003\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Dessa forma conseguimos reduzir nossa função em poucas iterações (rodar mais 2x)"
      ],
      "metadata": {
        "id": "jieswZFNcVgG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Passo maior nem sempre é melhor**: É importante destacar que qualquer valor de passo maior que 0.01 ia sempre funcionar melhor (por exemplo, passo = 1 gera a saída = 1). No entanto, à medida que os circuitos vão ficando mais complexos (como em redes neurais completas), a função vai ser tornando mais caótica e complexa. O gradiente garante que se você tem um passo muito pequeno (o ideal seria infinitesimal), então você definitivamente aumenta a saída seguindo aquela direção. O passo que estamos utilizando (0.01) ainda é muito grande, mas como nosso circuito é simples, podemos esperar pelo melhor resultado. Lembre-se da analogia do **escalador cego**."
      ],
      "metadata": {
        "id": "qO_-mHJQciad"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Estratégia 4: Gradiente Analítico"
      ],
      "metadata": {
        "id": "tRgGJWgJcmYN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A estratégia que utilizamos até agora de ajustar levemente a entrada e ver o que acontece com a saída pode não ser muito cômoda na prática quando temos milhares de entradas para ajustar. Então, a gente precisa de algo melhor.\n",
        "\n",
        "Felizmente, existe uma estratégia mais fácil e muito mais rápida para calcular o gradiente: podemos usar cálculo para derivar diretamente a nossa função. Chamamos isso de **gradiente analítico** e dessa forma não precisamos ajustar levemente nada. \n",
        "\n",
        "> *O gradiente analítico evita o leve ajustamento das entradas. O circuito pode ser derivado usando cálculo.*\n",
        "\n",
        "É muito fácil calcular derivadas parciais para funções simples como $x*y$. Se você não lembra da definição, aqui está o cálculo da derivada parcial em relação a $x$ da nossa função $f(x,y)$:\n",
        "\n",
        "$$\\frac{\\partial f(x,y)}{\\partial x} = \\frac{f(x+h,y) - f(x,y)}{h}\n",
        "= \\frac{(x+h)y - xy}{h}\n",
        "= \\frac{xy + hy - xy}{h}\n",
        "= \\frac{hy}{h}\n",
        "= y$$\n",
        "\n",
        "A derivada parcial em relação em $x$ da nossa $f(x,y)$ é igual $y$. Você reparou na coincidência de $\\partial x = 3.0$, que é exatamente o valor de $y$? E que o mesmo aconteceu para $x$? **Então, a gente não precisa ajustar nada!** E nosso código fica assim:"
      ],
      "metadata": {
        "id": "8P9_J5iPco9F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x, y = 1, -3\n",
        "out = forwardMultiplyGate(x,y)\n",
        "\n",
        "derivada_x = y\n",
        "derivada_y = x\n",
        "\n",
        "passo = 0.001\n",
        "\n",
        "x = x + passo * derivada_x\n",
        "y = y + passo * derivada_y\n",
        "\n",
        "out_new = forwardMultiplyGate(x,y)\n",
        "print(out_new)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2ZpPemgWc311",
        "outputId": "f9b50d90-5a45-4950-963d-6ce5bb6d459e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-2.990003\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "É importante destacar que a Estratégia #3 reduziu a #2 para uma única vez. Porém, a #3 nos dá somente uma aproximação do gradiente, enquanto a Estratégia #4 nos dá o valor exato. Sem aproximações. O único lado negativo é que temos de saber derivar a nossa funcão.\n",
        "\n",
        "Recapitulando o que vimos até aqui:\n",
        "- __Estratégia 1__: definimos valores aleatórios em todas as iterações. Não funciona para muitas entradas.\n",
        "- __Estratégia 2__: pequenos ajustes aleatórios nas entradas e vemos qual funciona melhor. Tão ruim quando a #1.\n",
        "- __Estratégia 3__: muito melhor através do cálculo do gradiente. Independentemente de quão complicado é o circuito, o **gradiente numérico** é muito simples de se calcular (mas um pouco caro).\n",
        "- __Estratégia 4__: no final, vimos que a forma melhor, mais inteligente e mais rápida é calcular o **gradiente analítico**. O resultado é idêntico ao gradiente numérico, porém mais rápido e não precisa de ajustes."
      ],
      "metadata": {
        "id": "jClYQKl_c_LW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Caso Recursivo: Múltiplas Portas"
      ],
      "metadata": {
        "id": "W6RTodD9fNZ-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Como vamos calcular agora a nossa derivada? Primeiramente, vamos esquecer da porta de soma e fingir que temos apenas duas entradas no nosso circuito: **q** e **z**. Como já vimos, as nossas derivadas parciais podem ser definidas da seguinte maneira:\n",
        "\n",
        "$$f(q,z) = q z \\hspace{0.5in} \\implies \\hspace{0.5in} \\frac{\\partial f(q,z)}{\\partial q} = z, \\hspace{1in} \\frac{\\partial f(q,z)}{\\partial z} = q$$\n",
        "\n",
        "Ok, mas e em relação a $x$ e $y$? Como $q$ é calculado em função de $x$ e $y$ (pela adição em nosso exemplo), nós também podemos calcular suas derivadas parciais:\n",
        "\n",
        "$$q(x,y) = x + y \\hspace{0.5in} \\implies \\hspace{0.5in} \\frac{\\partial q(x,y)}{\\partial x} = 1, \\hspace{1in} \\frac{\\partial q(x,y)}{\\partial y} = 1$$\n",
        "\n",
        "Correto! As derivadas parciais são 1, independentemente dos valores de $x$ e $y$. Isso faz sentido se pensarmos que para aumentar A saída de uma porta de adição, a gente espera uma força positiva tanto em $x$ quanto em $y$, independente dos seus valores.\n",
        "\n",
        "Com as fórmulas acima, nós sabemos calcular o gradiente da saída em relação a $q$ e $z$, e o gradiente de $q$ em relação a $x$ e $y$. Para calcular o gradiente do nosso circuito em relação a $x$, $y$ e $z$, nós vamos utilizar a **Regra da Cadeia**, que vai nos dizer como combinar esses gradientes. A derivada final em relação a $x$, será dada por:\n",
        "\n",
        "$$\\frac{\\partial f(q,z)}{\\partial x} = \\frac{\\partial q(x,y)}{\\partial x} \\frac{\\partial f(q,z)}{\\partial q}$$\n",
        "\n",
        "Pode parecer complicado à primeira vista, mas a verdade é que isso vai ser simplificado a somente duas multiplicações:"
      ],
      "metadata": {
        "id": "r33na5H5fRK-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def forwardAddGate(a, b):\n",
        "  return a + b\n",
        "\n",
        "def forwardCircuit(x, y, z):\n",
        "  q = forwardAddGate(x, y)\n",
        "  f = forwardMultiplyGate(q, z)\n",
        "  return f\n",
        "\n",
        "print(forwardCircuit(-2, 5, -4)) # -12\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x9gKhcI5fShU",
        "outputId": "208143d2-a4e9-475f-b2e4-19af321a4b7f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "12\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x, y, z = -2, 5, -4\n",
        "q = forwardAddGate(x, y)\n",
        "f = forwardMultiplyGate(q, z)\n",
        "# Derivada da porta de multiplicação\n",
        "der_f_z = q\n",
        "der_f_q = z\n",
        "# Derivada da porta de adição\n",
        "der_q_x = 1\n",
        "der_q_y = 1\n",
        "\n",
        "# Regra da cadeia\n",
        "der_f_x = der_f_q * der_q_x\n",
        "der_f_y = der_f_q * der_q_y\n",
        "\n",
        "print(der_f_x, der_f_y, der_f_z) # tem que bater com slide"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oo3gw2l4fvUV",
        "outputId": "79c78c58-fe64-4534-d95d-7ab0c934cf65"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-4 -4 3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "O sinal negativo nos indica se devemos crescer ou decrescer aquela entrada\n",
        "e o valor indica com que força"
      ],
      "metadata": {
        "id": "jpV_BbRsm8nW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Agora atualizaremos os nossos pesos com o gradiente"
      ],
      "metadata": {
        "id": "4RFc1X7hmbXk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "grad_f_rel_xyz = [der_f_x, der_f_y, der_f_z] # gradiente!!!\n",
        "\n",
        "passo = 0.01\n",
        "x =x + passo * der_f_x\n",
        "y = y + passo * der_f_y\n",
        "z = z + passo * der_f_z\n",
        "#esse valor deverá ser maior que -12\n",
        "print(forwardCircuit(x, y, z))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hoeJmiHRmEzY",
        "outputId": "2ad05322-3879-47f4-ec85-c3e9b9a6ff0e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-10.7916\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "WWoRWw8zn0M-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Checagem do gradiente numérico"
      ],
      "metadata": {
        "id": "bSLNnDRwn3am"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Vamos verificar se os gradientes analíticos que calculamos por backpropagation estão corretos. Lembre-se que podemos fazer isso através do gradiente numérico e esperamos que o resultado seja [-4, -4, 4] para $x,y,z$."
      ],
      "metadata": {
        "id": "C6fjb411n41O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x,y,z = -2,5,-4\n",
        "h = 0.0001\n",
        "\n",
        "der_x = (forwardCircuit(x + h, y, z) - forwardCircuit(x , y, z)) / h\n",
        "der_y = (forwardCircuit(x, y+ h, z) - forwardCircuit(x , y, z)) / h\n",
        "der_z = (forwardCircuit(x, y, z+ h) - forwardCircuit(x , y, z)) / h\n",
        "\n",
        "print(\"Esperamos que este resultado fique proximos dos resultados anteriores, estamos supondo que nao sabemos as derivadas\")\n",
        "\n",
        "print(der_x, der_y, der_z)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "56HyHwaon6Ul",
        "outputId": "bdaa7cf5-de8c-4735-e72a-95e0df9a68b3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Esperamos que este resultado fique proximos dos resultados anteriores, estamos supondo que nao sabemos as derivadas\n",
            "-3.9999999999906777 -3.9999999999906777 3.000000000010772\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Neurônio Sigmóide"
      ],
      "metadata": {
        "id": "l10Jczi5I0VU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Qualquer função diferenciável pode atuar como uma porta, como também podemos agrupar múltiplas portas para formar uma simples porta, ou decompor um função em múltiplas portas quando for conveniente. Para exemplificar, vamos utilizar a função de ativação *sigmoid* com entradas **x** e pesos **w**:\n",
        "> Neste caso o bias é representado pelo w2\n",
        "\n",
        "$$f(w,x) = \\frac{1}{1+e^{-(w_0x_0 + w_1x_1 + w_2)}}$$\n",
        "\n",
        "Como dito, a função acima nada mais é que a função sigmoid $\\sigma(x)$. Sabendo, então, que a derivada da função sigmoid é:\n",
        "\n",
        "$$\\sigma(x)=\\frac{1}{1+e^{-x}}=(1-\\sigma(x))\\sigma(x)$$\n",
        "\n",
        "Vamos calcular a gradiente em relação as entradas:"
      ],
      "metadata": {
        "id": "e0WS2wJiIydl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![This is an image](https://github.com/mafaldasalomao/pavic_treinamento_ml/blob/main/Machine_Learning/figures/sigmoid_example_backprop.png?raw=true)"
      ],
      "metadata": {
        "id": "Oius3x-NA5tH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "w0, w1, w2 = 2, -3, -3\n",
        "x0, x1 = -1, -2\n",
        "\n",
        "# forward pass\n",
        "mult1 = x0 * w0\n",
        "mult2 = x1 * w1\n",
        "add1 = mult1 + mult2\n",
        "add2 = add1 + w2\n",
        "neg = -1.0 * add2\n",
        "exp1 = np.exp(neg)\n",
        "add3 = 1.0 + exp1\n",
        "f = 1.0 / add3\n",
        "print(mult1, mult2, add1, add2, neg, exp1, add3, f)\n",
        "print(\"===============================================\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "47pEjlp4B70o",
        "outputId": "d59a863b-c1a6-455d-c45c-5edcbfb9f8fc"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-2 6 4 1 -1.0 0.36787944117144233 1.3678794411714423 0.7310585786300049\n",
            "===============================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#backward pass - de baixo para cima\n",
        "df = +1.0 \n",
        "dadd3 = -1.0/(add3 * add3) * df #deriva o de dentro e multiplica pela derivada de quem ta fora\n",
        "dexp1 = 1.0 * dadd3\n",
        "dneg = np.exp(neg) * dexp1\n",
        "dadd2 = -1.0 * dneg\n",
        "dw2 = 1.0 * dadd2\n",
        "dadd1 = 1.0 * dadd2\n",
        "dmult1 = 1.0 * dadd1\n",
        "dmult2 = 1.0 * dadd1\n",
        "dx1 = w1 * dmult2\n",
        "dw1 = x1 * dmult2\n",
        "dx0 = w0 * dmult1\n",
        "dw0 = x0* dmult1\n",
        "\n",
        "print(dw0, dx0, dw1, dx1, dmult2, dmult1, dadd1, dw2,dadd2, dneg, dexp1, dadd3, df, sep='\\n')\n",
        "print(\"============================================================\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6wIKELeMCu9f",
        "outputId": "3b01d515-e9bd-43d1-e6b9-849a5827cf5b"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-0.19661193324148188\n",
            "0.39322386648296376\n",
            "-0.39322386648296376\n",
            "-0.5898357997244457\n",
            "0.19661193324148188\n",
            "0.19661193324148188\n",
            "0.19661193324148188\n",
            "0.19661193324148188\n",
            "0.19661193324148188\n",
            "-0.19661193324148188\n",
            "-0.534446645388523\n",
            "-0.534446645388523\n",
            "1.0\n",
            "============================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "w0, w1, w2 = 2, -3, -3\n",
        "x0, x1 = -1, -2\n",
        "\n",
        "# forward pass\n",
        "dot = w0 * x0 + w1*x1 + w2\n",
        "f = 1.0/ (1.0 + np.exp(-dot))\n",
        "\n",
        "# backward pass, derivamos de baixo para cima\n",
        "df = +1.0 # função de custo, saida\n",
        "ddot = ((1-f)*f)*df #derivada da sigmoid\n",
        "\n",
        "dx0 = w0 * ddot\n",
        "dw0 = x0 * ddot\n",
        "dx1 = w1 * ddot\n",
        "dw1 = x1 * ddot\n",
        "dw2 = 1.0 * ddot\n",
        "print('dx0: {} dx1: {}'.format(dx0, dx1))\n",
        "print('dw0: {} dw1: {}'.format(dw0, dw1))\n",
        "# Nova saida\n",
        "step = 0.001\n",
        "w0 = w0 + step * dw0\n",
        "w1 = w1 + step * dw1\n",
        "x0 = x0 + step * dx0\n",
        "x1 = x1 + step * dx1\n",
        "\n",
        "new_dot = w0 * x0 + w1*x1 + w2\n",
        "new_f = 1.0 /(1.0+np.exp(-new_dot))\n",
        "print(f, new_f)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NqHO4fTIJH78",
        "outputId": "cd78dcb7-1fd7-490f-a7bb-78b183836c28"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dx0: 0.3932238664829637 dx1: -0.5898357997244456\n",
            "dw0: -0.19661193324148185 dw1: -0.3932238664829637\n",
            "0.7310585786300049 0.7317538522828431\n"
          ]
        }
      ]
    }
  ]
}