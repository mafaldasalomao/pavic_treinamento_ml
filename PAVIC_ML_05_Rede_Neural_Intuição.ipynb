{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mafaldasalomao/pavic_treinamento_ml/blob/main/PAVIC_ML_05_Rede_Neural_Intui%C3%A7%C3%A3o.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dNxQ27HCKBQb"
      },
      "source": [
        "Neste notebook, vamos codificar Redes Neurais de forma manual para tentar entender intuitivamente como elas são implementadas na prática."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "scLxcHutKBQc"
      },
      "source": [
        "# Sumário"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zU22HLr9KBQd"
      },
      "source": [
        "- [Exemplo 1](#Exemplo-1)\n",
        "- [Exemplo 2](#Exemplo-2)\n",
        "- [O que precisamos para implementar uma Rede Neural?](#O-que-precisamos-para-implementar-uma-Rede-Neural?)\n",
        "- [Referências](#Referências)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PMTztVO5KBQd"
      },
      "source": [
        "# Imports e Configurações"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "BwJu6GFAKBQd"
      },
      "outputs": [],
      "source": [
        "import numpy as np"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y0FmgOYpKBQe"
      },
      "source": [
        "# Exemplo 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DDtuh6i6KBQe"
      },
      "source": [
        "<img src='https://github.com/mafaldasalomao/pavic_treinamento_ml/blob/main/Machine_Learning/figures/backprop_example_1.png?raw=true'>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "-QGL_lG-KBQe"
      },
      "outputs": [],
      "source": [
        "def sigmoid(x, derivative=False):\n",
        "  if derivative:\n",
        "    y = sigmoid(x)\n",
        "    return y * (1 - y)\n",
        "  return 1.0 /( 1.0 + np.exp(-x))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "PYqm6OsPKBQe"
      },
      "outputs": [],
      "source": [
        "x = np.array([[0.05, 0.10]])\n",
        "y = np.array([[0.01, 0.99]])\n",
        "#pesos\n",
        "w1 = np.array([[0.15, 0.20], [0.25, 0.30]])\n",
        "#neste exemplo esta sendo usado um bia pra dois neuronios\n",
        "b1 = np.array([[0.35]])\n",
        "\n",
        "w2 = np.array([[0.40, 0.45], [0.50, 0.55]])\n",
        "b2 = np.array([[0.60]])\n",
        "\n",
        "learning_rate = 0.5"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Forward"
      ],
      "metadata": {
        "id": "7zLntV8AsUy0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Forward\n",
        "for i in range(1):\n",
        "  #feed-forward\n",
        "  #1 camada\n",
        "  inp1 = np.dot(x, w1.T) + b1\n",
        "  h1 = sigmoid(inp1)\n",
        "  #2 camada\n",
        "  inp2 = np.dot(h1, w2.T) + b2\n",
        "  out = sigmoid(inp2)\n",
        "\n",
        "  cost = 0.5 * np.sum((y-out)**2)\n",
        "\n",
        "print(h1)\n",
        "print(out)\n",
        "print(cost)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oeXTB17UsTEv",
        "outputId": "dbd33b93-0100-4f15-ac71-fa96cebd5b7d"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0.59326999 0.59688438]]\n",
            "[[0.75136507 0.77292847]]\n",
            "0.2983711087600027\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## BackProp"
      ],
      "metadata": {
        "id": "32voAGlRtQTE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#desta vez o custo nao será 1\n",
        "dout = -(y-out) # derivada da funçao de custo\n",
        "#2a derivada\n",
        "dinp2 = sigmoid(inp2, derivative=True) * dout\n",
        "# VER NO SLIDE DE DERIVAÇÂO DE MATRIZES\n",
        "dh1 = np.dot(dinp2, w2)\n",
        "dw2 = np.dot(dinp2.T, h1)\n",
        "db2 = 1.0 * dinp2.sum(axis=0, keepdims=True) # temos que adicionar do eixo 0 para somar  Senao perde a dimensionalidade\n",
        "# 1 camada back\n",
        "dinp1 = sigmoid(inp1, derivative=True) * dh1\n",
        "dx = np.dot(dinp1, w1)\n",
        "dw1 = np.dot(dinp1.T, x)\n",
        "db1 = 1.0 * dinp1.sum(axis=0, keepdims=True)\n",
        "\n",
        "print(dw1)\n",
        "print(dw2)\n",
        "\n",
        "#Agora vamos atualizar os pesos\n",
        "w2 = w2 - learning_rate*dw2\n",
        "b2 = b2 - learning_rate*b2\n",
        "w1 = w1 - learning_rate*dw1\n",
        "b1 = b1 - learning_rate*b1\n",
        "#observe que os valores dos pesos foram modificados de maneira suave\n",
        "print(w1)\n",
        "print(w2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0tZUK3MUtTDG",
        "outputId": "4e3fa36f-4a73-4cbb-c68e-fcaea46778ef"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0.00043857 0.00087714]\n",
            " [0.00049771 0.00099543]]\n",
            "[[ 0.08216704  0.08266763]\n",
            " [-0.02260254 -0.02274024]]\n",
            "[[0.14978072 0.19956143]\n",
            " [0.24975114 0.29950229]]\n",
            "[[0.35891648 0.40866619]\n",
            " [0.51130127 0.56137012]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rli2tpmZKBQe"
      },
      "source": [
        "# Exemplo 2"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<img src=\"https://miro.medium.com/v2/resize:fit:828/format:webp/1*fnU_3MGmFp0LBIzRPx42-w.png\"/>\n"
      ],
      "metadata": {
        "id": "GstWpODDF_Js"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<img ulr=https://miro.medium.com/v2/resize:fit:828/format:webp/1*fnU_3MGmFp0LBIzRPx42-w.png>"
      ],
      "metadata": {
        "id": "-nS3dIDOF6H0"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "1CmSQgtfKBQe"
      },
      "outputs": [],
      "source": [
        "def linear(x, derivative=False):\n",
        "    return np.ones_like(x) if derivative else x\n",
        "\n",
        "def relu(x, derivative=False):\n",
        "    if derivative:\n",
        "        x = np.where(x <= 0, 0, 1)\n",
        "    return np.maximum(0, x)\n",
        "\n",
        "def softmax(x, y_oh=None, derivative=False): #y_one-hot\n",
        "  if derivative:\n",
        "    y_pred = softmax(x)\n",
        "    k = np.nonzero(y_pred * y_oh)\n",
        "    pk = y_pred[k]\n",
        "    y_pred[k] = pk * (1.0 - pk)\n",
        "    return y_pred\n",
        "  exp = np.exp(x)\n",
        "  return exp / np.sum(exp,  axis=1, keepdims=True) # vamos somar liha por linha das exp\n",
        "\n",
        "def neg_log_likelihood(y_oh, y_pred, derivative=False):\n",
        "  k = np.nonzero(y_pred * y_oh)\n",
        "  pk = y_pred[k]\n",
        "  if derivative:\n",
        "    y_pred[k] = (-1.0 / pk)\n",
        "    return y_pred\n",
        "  return np.mean(-np.log(pk))\n",
        "\n",
        "def softmax_neg_log_likelihood(y_oh, y_pred, derivative=False):\n",
        "  y_softmax = softmax(y_pred)\n",
        "  if derivative:\n",
        "    k = np.nonzero(y_pred * y_oh) # pegar os campos reais de saída\n",
        "    dlog = neg_log_likelihood(y_oh, y_softmax, derivative=True) #pegar a derivada de neg\n",
        "    dsoftmax = softmax(y_pred, y_oh, derivative=True) # derivada da soft\n",
        "    y_softmax[k] = dlog[k] * dsoftmax[k] # multiplicamos a derivada de dlog em relação a pk * derivada soft em relação a y\n",
        "    return y_softmax / y_softmax.shape[0] #1/N\n",
        "  return neg_log_likelihood(y_oh, y_softmax)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "NNQjLLA3KBQe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "efaa7c33-a8d8-4ac9-ef2b-69346e8f03c4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1.1674456052871238\n",
            "0.6079795820435694\n",
            "0.36558760028370507\n",
            "0.24943451548999263\n",
            "0.18547915491555542\n",
            "0.14614614804167658\n",
            "0.1199051550833629\n",
            "0.10131084011234358\n",
            "0.08751844300750028\n",
            "0.07691720650783256\n",
            "0.06853445083252653\n",
            "[[0.10083595 0.2016719  0.30585165]\n",
            " [0.30086971 0.20173942 0.70608796]\n",
            " [0.40145052 0.30290104 0.91015363]]\n",
            "[[0.20544723 0.30673159 0.50749567]\n",
            " [0.30994562 0.5123005  0.71366784]\n",
            " [0.61065514 0.41317913 0.81464085]]\n",
            "[[ 0.66465527  0.98758148  1.39393956]\n",
            " [ 0.05020341  0.44006253 -0.06274803]\n",
            " [ 0.18514132 -0.12764401  0.56880846]]\n"
          ]
        }
      ],
      "source": [
        "x = np.array([[0.1, 0.2, 0.7]])\n",
        "y = np.array([[1, 0, 0]])\n",
        "w1 = np.array([[0.1, 0.2, 0.3], [0.3, 0.2, 0.7], [0.4, 0.3, 0.9]])\n",
        "b1 = np.ones((1,3))\n",
        "w2 = np.array([[0.2, 0.3, 0.5], [0.3, 0.5, 0.7], [0.6, 0.4, 0.8]])\n",
        "b2 = np.ones((1,3))\n",
        "w3 = np.array([[0.1, 0.4, 0.8], [0.3, 0.7, 0.2], [0.5, 0.2, 0.9]])\n",
        "b3 = np.ones((1,3))\n",
        "\n",
        "learning_rate = 0.01\n",
        "\n",
        "for i in range(301):\n",
        "    # feedforward\n",
        "    # 1a camada\n",
        "    inp1 = np.dot(x, w1.T) + b1\n",
        "    h1 = relu(inp1)\n",
        "    # 2a camada\n",
        "    inp2 = np.dot(h1, w2.T) + b2\n",
        "    h2 = sigmoid(inp2)\n",
        "  \n",
        "    # 3a camada\n",
        "    inp3 = np.dot(h2, w3.T) + b3\n",
        "    out = linear(inp3)\n",
        "\n",
        "    cost = softmax_neg_log_likelihood(y, out)\n",
        "    # backpropagation\n",
        "    # insira seu código aqui!\n",
        "    dout = softmax_neg_log_likelihood(y, out, derivative=True)\n",
        "\n",
        "    #3a camada\n",
        "    dinp3 = linear(inp3, derivative=True) * dout\n",
        "    dh2 = np.dot(dinp3, w3)\n",
        "    dw3 = np.dot(dinp3.T, h2)\n",
        "    db3 = 1.0 * dinp3.sum(axis=0, keepdims=True) \n",
        "\n",
        "    #2a camada\n",
        "    dinp2 = sigmoid(inp2, derivative=True) * dh2\n",
        "    dh1 = np.dot(dinp2, w2)\n",
        "    dw2 = np.dot(dinp2.T, h1)\n",
        "    db2 = 1.0 * dinp2.sum(axis=0, keepdims=True) \n",
        "\n",
        "    #1a camada\n",
        "    dinp1 = relu(inp1, derivative=True) * dh1\n",
        "    dx = np.dot(dinp1, w1)\n",
        "    dw1 = np.dot(dinp1.T, x)\n",
        "    db1 = 1.0 * dinp1.sum(axis=0, keepdims=True) \n",
        "\n",
        "\n",
        "    #Vamos atualizar os pesos\n",
        "    w3 = w3 - learning_rate * dw3\n",
        "    b3 = b3 - learning_rate * db3\n",
        "    w2 = w2 - learning_rate * dw2\n",
        "    b2 = b2 - learning_rate * db2\n",
        "    w1 = w1 - learning_rate * dw1\n",
        "    b1 = b1 - learning_rate * db1\n",
        "\n",
        "    if i % 30 ==0:\n",
        "      cost = softmax_neg_log_likelihood(y, out)\n",
        "      print(cost)\n",
        "for w in [w1, w2, w3]:\n",
        "    print(w)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "smO3Nrd8KBQf"
      },
      "source": [
        "# O que precisamos para implementar uma Rede Neural?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uNmDZIU9KBQf"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KH7pa_H1KBQf"
      },
      "source": [
        "# Referências"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iB6uGBsMKBQf"
      },
      "source": [
        "- [Neural Network from Scratch](https://beckernick.github.io/neural-network-scratch/)\n",
        "- [Backpropagation Algorithm](https://theclevermachine.wordpress.com/tag/backpropagation-algorithm/)\n",
        "- [Back-Propagation is very simple. Who made it Complicated ?](https://becominghuman.ai/back-propagation-is-very-simple-who-made-it-complicated-97b794c97e5c)\n",
        "- [A Step by Step Backpropagation Example](https://mattmazur.com/2015/03/17/a-step-by-step-backpropagation-example/)\n",
        "- [Understanding softmax and the negative log-likelihood](https://ljvmiranda921.github.io/notebook/2017/08/13/softmax-and-the-negative-log-likelihood/)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.5"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}